{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7893d5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Once data has finished processing, load data from folder structure\n",
    "# text-data/\n",
    "# ├─ advertisement/\n",
    "# ├─ email/\n",
    "# ├─ invoice/\n",
    "# ....\n",
    "data = load_files('../text-data', encoding='utf-8', decode_error='ignore')\n",
    "\n",
    "X = data.data               \n",
    "y = data.target             \n",
    "class_names = data.target_names \n",
    "\n",
    "# split data for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64feffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize text using TF-IDF\n",
    "vectorizer = TfidfVectorizer(\n",
    "    # this is all first pass so might want to update after check in\n",
    "    # limit vocab\n",
    "    max_features=5000,\n",
    "    # ignore terms that appear in fewer than 2 documents     \n",
    "    min_df=2,               \n",
    "    # ignore terms that appear in more than 80% of documents\n",
    "    max_df=0.8,\n",
    "    # using unigrams and bigrams             \n",
    "    ngram_range=(1, 2), \n",
    "    # removing common English words    \n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "X_train_vectors = vectorizer.fit_transform(X_train)\n",
    "X_test_vectors = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a7c2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train baseline Logistic Regression\n",
    "base_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "base_model.fit(X_train_vectors, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb81f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval on test set\n",
    "y_pred = base_model.predict(X_test_vectors)\n",
    "print(f\"Test Accuracy: {base_model.score(X_test_vectors, y_test)}\\n\")\n",
    "\n",
    "print(\"Baseline Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8368f4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning with grid search\n",
    "\n",
    "# chains vectorizer and model\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words='english')),\n",
    "    ('clf', LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    # max vocab size\n",
    "    'tfidf__max_features': [3000, 5000, 7000],\n",
    "    # unigrams only, unigrams + bigrams, unigrams + bigrams + trigrams\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    # min document frequency - removes rare words\n",
    "    'tfidf__min_df': [2, 3, 5],\n",
    "    # max document frequency - removes common words\n",
    "    'tfidf__max_df': [0.7, 0.8, 0.9],\n",
    "    # inverse regularization strength\n",
    "    'clf__C': [0.1, 1, 10],\n",
    "    # optimization algo\n",
    "    'clf__solver': ['liblinear', 'lbfgs'],\n",
    "    # L2 regularization\n",
    "    'clf__penalty': ['l2']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc10a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate tuned model\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_tuned = best_model.predict(X_test)\n",
    "print(f\"Tuned Test Accuracy: {best_model.score(X_test, y_test)}\")\n",
    "\n",
    "print(\"Tuned Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_tuned, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae9a336",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred_tuned)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_names, yticklabels=class_names, cmap='Blues')\n",
    "plt.title(\"Confusion Matrix (Tuned LR)\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.savefig('confusion_matrix_tuned_lr.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8661feb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature analysis\n",
    "tuned_vectorizer = best_model.named_steps['tfidf']\n",
    "tuned_clf = best_model.named_steps['clf']\n",
    "feature_names = np.array(tuned_vectorizer.get_feature_names_out())\n",
    "\n",
    "top_n = 10\n",
    "for i, class_label in enumerate(class_names):\n",
    "    # get and print top weights and features\n",
    "    coefficient = tuned_clf.coef_[i]\n",
    "    top_indices = np.argsort(coefficient)[-top_n:]\n",
    "    top_features = feature_names[top_indices]\n",
    "    top_weights = coefficient[top_indices]\n",
    "    print(f\"Top features for class '{class_label}':\")\n",
    "    for feature, weight in zip(top_features, top_weights):\n",
    "        print(f\"{feature}: {weight:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae6b02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "import joblib\n",
    "joblib.dump(best_model, 'lr_tuned.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
