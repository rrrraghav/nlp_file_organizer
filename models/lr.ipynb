{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7893d5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Once data has finished processing, load data from folder structure\n",
    "# text-data/\n",
    "# ├─ advertisement/\n",
    "# ├─ email/\n",
    "# ├─ invoice/\n",
    "# ....\n",
    "data = load_files('../text-data', encoding='utf-8', decode_error='ignore')\n",
    "\n",
    "X = data.data               \n",
    "y = data.target             \n",
    "class_names = data.target_names \n",
    "\n",
    "# split data for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64feffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize text using TF-IDF\n",
    "vectorizer = TfidfVectorizer(\n",
    "    # this is all first pass so might want to update after check in\n",
    "    # limit vocab\n",
    "    max_features=5000,\n",
    "    # ignore terms that appear in fewer than 2 documents     \n",
    "    min_df=2,               \n",
    "    # ignore terms that appear in more than 80% of documents\n",
    "    max_df=0.8,\n",
    "    # using unigrams and bigrams             \n",
    "    ngram_range=(1, 2), \n",
    "    # removing common English words    \n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "X_train_vectors = vectorizer.fit_transform(X_train)\n",
    "X_test_vectors = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a7c2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train baseline Logistic Regression\n",
    "base_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "base_model.fit(X_train_vectors, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb81f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval on test set\n",
    "y_pred = base_model.predict(X_test_vectors)\n",
    "print(f\"Test Accuracy: {base_model.score(X_test_vectors, y_test)}\\n\")\n",
    "\n",
    "print(\"Baseline Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8368f4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning with grid search\n",
    "\n",
    "# chains vectorizer and model\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words='english')),\n",
    "    ('clf', LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    # max vocab size\n",
    "    'tfidf__max_features': [3000, 5000, 7000],\n",
    "    # unigrams only, unigrams + bigrams, unigrams + bigrams + trigrams\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    # min document frequency - removes rare words\n",
    "    'tfidf__min_df': [2, 3, 5],\n",
    "    # max document frequency - removes common words\n",
    "    'tfidf__max_df': [0.7, 0.8, 0.9],\n",
    "    # inverse regularization strength\n",
    "    'clf__C': [0.1, 1, 10],\n",
    "    # optimization algo\n",
    "    'clf__solver': ['saga', 'lbfgs'],\n",
    "    # L2 regularization\n",
    "    'clf__penalty': ['l2']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc10a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate tuned model\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_tuned = best_model.predict(X_test)\n",
    "print(f\"Tuned Test Accuracy: {best_model.score(X_test, y_test)}\")\n",
    "\n",
    "print(\"Tuned Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_tuned, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae9a336",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred_tuned)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_names, yticklabels=class_names, cmap='Blues')\n",
    "plt.title(\"Confusion Matrix (Tuned LR)\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.savefig('confusion_matrix_tuned_lr.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b35ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get misclassified samples\n",
    "y_pred_tuned = best_model.predict(X_test)\n",
    "misclassified_mask = y_test != y_pred_tuned\n",
    "misclassified_indices = np.where(misclassified_mask)[0]\n",
    "\n",
    "misclass_df = pd.DataFrame({\n",
    "    'true_label': [class_names[y_test[i]] for i in misclassified_indices],\n",
    "    'predicted_label': [class_names[y_pred_tuned[i]] for i in misclassified_indices]\n",
    "})\n",
    "\n",
    "misclass_pairs = misclass_df.groupby(['true_label', 'predicted_label']).size().reset_index(name='count')\n",
    "misclass_pairs = misclass_pairs.sort_values('count', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_pairs = misclass_pairs.head(10)\n",
    "plt.barh(range(len(top_pairs)), top_pairs['count'], color='#3498db')\n",
    "plt.yticks(range(len(top_pairs)), \n",
    "           [f\"{row['true_label']} → {row['predicted_label']}\" \n",
    "            for _, row in top_pairs.iterrows()])\n",
    "plt.xlabel('Number of Misclassifications', fontsize=12)\n",
    "plt.title('Top 10 Misclassification Patterns (LR)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('presentation_misclass_patterns_lr.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "error_by_class = pd.DataFrame({\n",
    "    'Class': class_names,\n",
    "    'Total': [np.sum(y_test == i) for i in range(len(class_names))],\n",
    "    'Errors': [np.sum((y_test == i) & (y_pred_tuned != i)) for i in range(len(class_names))],\n",
    "})\n",
    "error_by_class['Error Rate (%)'] = (error_by_class['Errors'] / error_by_class['Total'] * 100).round(1)\n",
    "error_by_class = error_by_class.sort_values('Error Rate (%)', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['#e74c3c' if x > error_by_class['Error Rate (%)'].median() else '#3498db' \n",
    "          for x in error_by_class['Error Rate (%)']]\n",
    "plt.barh(error_by_class['Class'], error_by_class['Error Rate (%)'], color=colors)\n",
    "plt.xlabel('Error Rate (%)', fontsize=12)\n",
    "plt.title('Classification Error Rate by Class (LR)', fontsize=14, fontweight='bold')\n",
    "plt.axvline(error_by_class['Error Rate (%)'].median(), color='gray', linestyle='--', \n",
    "            linewidth=1, label='Median')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('presentation_error_by_class_lr.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8661feb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature analysis\n",
    "tuned_vectorizer = best_model.named_steps['tfidf']\n",
    "tuned_clf = best_model.named_steps['clf']\n",
    "feature_names = np.array(tuned_vectorizer.get_feature_names_out())\n",
    "\n",
    "top_n = 10\n",
    "for i, class_label in enumerate(class_names):\n",
    "    # get and print top weights and features\n",
    "    coefficient = tuned_clf.coef_[i]\n",
    "    top_indices = np.argsort(coefficient)[-top_n:]\n",
    "    top_features = feature_names[top_indices]\n",
    "    top_weights = coefficient[top_indices]\n",
    "    print(f\"Top features for class '{class_label}':\")\n",
    "    for feature, weight in zip(top_features, top_weights):\n",
    "        print(f\"{feature}: {weight:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae6b02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "import joblib\n",
    "joblib.dump(best_model, 'lr_tuned.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP Project",
   "language": "python",
   "name": "nlp_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
