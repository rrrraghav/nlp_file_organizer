{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03866686",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62cd420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once data has finished processing, load data from folder structure\n",
    "# text-data/\n",
    "# ├─ advertisement/\n",
    "# ├─ email/\n",
    "# ├─ invoice/\n",
    "# ....\n",
    "data = load_files('../text-data', encoding='utf-8', decode_error='ignore')\n",
    "\n",
    "X = data.data               \n",
    "y = data.target             \n",
    "class_names = data.target_names \n",
    "\n",
    "# split data for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bda9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize tokenizer and model\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "def model_init(trial):\n",
    "    \"creates a new RoBERTa model each run\"\n",
    "    return RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels = len(class_names))\n",
    "\n",
    "\n",
    "# gives the options of value for each run -- to find the best on\n",
    "def hyperparameters(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [8, 16, 32]),\n",
    "        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 3, 10),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 0.0, 0.1),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469e89a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the data to string\n",
    "def convert_byte_to_string(data):\n",
    "    list_text = []\n",
    "    for text in data:\n",
    "        if isinstance(text, bytes):\n",
    "            list_text.append(text.decode('utf-8'))\n",
    "        else:\n",
    "            #not byte, must be a string\n",
    "            list_text.append(text)\n",
    "    return list_text\n",
    "\n",
    "# Tokenize with map function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "X_train_convert = convert_byte_to_string(X_train)\n",
    "X_test_convert = convert_byte_to_string(X_test)\n",
    "\n",
    "train_data = Dataset.from_dict({\n",
    "    'text': X_train_convert,\n",
    "    'labels': y_train.tolist()\n",
    "})\n",
    "\n",
    "test_data = Dataset.from_dict({\n",
    "    'text': X_test_convert,\n",
    "    'labels': y_test.tolist()\n",
    "})\n",
    "\n",
    "train_dataset = train_data.map(tokenize_function, batched=True)\n",
    "test_dataset = test_data.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ace85f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training location\n",
    "output_dir = '../models/RoBERTa-data'  \n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='no',\n",
    "    load_best_model_at_end=True,\n",
    "    logging_dir=f'{output_dir}/logs',\n",
    "    logging_steps=10,\n",
    "    report_to='none'\n",
    ")\n",
    "\n",
    "# Use with Trainer directly\n",
    "roberta_trainer = Trainer(\n",
    "    model=model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer = tokenizer\n",
    ")\n",
    "\n",
    "best_roberta_trial = roberta_trainer.hyperparameter_search(\n",
    "    direction=\"maximize\",\n",
    "    hp_space=hyperparameters,\n",
    "    n_trials=15,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a5ea5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final training arguments with the best hyperparameters\n",
    "# the values in each variable are a fallback incase something goes wrong\n",
    "final_training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=best_roberta_trial.hyperparameters.get('num_train_epochs', 10),\n",
    "    per_device_train_batch_size=best_roberta_trial.hyperparameters.get('per_device_train_batch_size', 16),\n",
    "    per_device_eval_batch_size=best_roberta_trial.hyperparameters.get('per_device_train_batch_size', 16),\n",
    "    learning_rate=best_roberta_trial.hyperparameters.get('learning_rate', 2e-5),\n",
    "    weight_decay=best_roberta_trial.hyperparameters.get('weight_decay', .001),\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_loss',\n",
    "    logging_dir=f'{output_dir}/final_roberta_model/logs',\n",
    "    logging_steps=10,\n",
    "    report_to='none',\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# used as final run\n",
    "final_roberta_model = RobertaForSequenceClassification.from_pretrained(\n",
    "    \"roberta-base\", \n",
    "    num_labels=len(class_names)\n",
    ")\n",
    "\n",
    "best_roberta_trainer = Trainer(\n",
    "    model=final_roberta_model,\n",
    "    args=final_training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# training\n",
    "best_roberta_trainer.train()\n",
    "\n",
    "# Save the final model\n",
    "best_roberta_trainer.save_model(f'{output_dir}/final_roberta_model')\n",
    "tokenizer.save_pretrained(f'{output_dir}/final_roberta_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a3ed6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "predictions = roberta_trainer.predict(test_dataset)\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "labels = predictions.label_ids\n",
    "accuracy = (preds == labels).mean()\n",
    "\n",
    "print(f\"Accuracy: {accuracy*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ab2d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # evaluate model\n",
    "final_roberta_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0db7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# graphs\n",
    "plt.figure(figsize=(16, 12))\n",
    "cm = confusion_matrix(labels, preds)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title(f'Confusion Matrix - RoBERTa Model\\nAccuracy: {accuracy*100:.2f}%')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../models/RoBERTa-data/confusion_matrix.png', dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP Project",
   "language": "python",
   "name": "nlp_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
